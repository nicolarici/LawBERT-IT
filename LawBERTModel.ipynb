{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6116,"status":"ok","timestamp":1666087822858,"user":{"displayName":"NICOLA ARICI","userId":"02667990344535794481"},"user_tz":-120},"id":"mwc03kM-fGkU","outputId":"ab16f13d-ce08-4e9f-9953-bc777470b06f"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to\n","[nltk_data]     /home/ec2-user/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import AutoTokenizer, AutoModel, BertTokenizer, BertForPreTraining\n","import nltk\n","from nltk.corpus import stopwords\n","import pandas as pd\n","from tqdm import tqdm\n","import re\n","import string\n","import torch\n","import torch.optim as optim\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')"]},{"cell_type":"markdown","metadata":{"id":"swkQuU5_ewWt"},"source":["## ***Creazione tokenizzatore LawBERT-IT***"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1666087860527,"user":{"displayName":"NICOLA ARICI","userId":"02667990344535794481"},"user_tz":-120},"id":"k-9T2rKSRuhn"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at dbmdz/bert-base-italian-xxl-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["# Carico BERT base\n","\n","tokenizer = AutoTokenizer.from_pretrained('dbmdz/bert-base-italian-xxl-uncased')  \n","model = AutoModel.from_pretrained('dbmdz/bert-base-italian-xxl-uncased')"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"BnxtlLHCG_-N"},"outputs":[],"source":["def parse_text(text):\n","\n","    # Remove number and date\n","    text = re.sub(r'[0-9]', ' ', text)\n","    text = re.sub('\\d+\\/\\d+\\/\\d+', ' ', text)\n","\n","    # Remove stopwords\n","    sent_text = nltk.word_tokenize(text.lower())\n","    return [word for word in sent_text if not word in stopwords.words('italian')]"]},{"cell_type":"markdown","metadata":{"id":"CKTYn5e2CjUX"},"source":["-> Recupero dati <br />\n","-> Tokenizzazione frasi in parole <br />\n","-> Controllo che siano nel vocabolario <br />\n","-> Aggiunta se compaiono almeno 50 volte"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":144503,"status":"ok","timestamp":1653552407721,"user":{"displayName":"NICOLA ARICI","userId":"02667990344535794481"},"user_tz":-120},"id":"0Jr6f_tWDYI2","outputId":"06ffed5e-5259-4569-8b4b-fdc4c8ac496a"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 702/702 [02:08<00:00,  5.46it/s]\n"]}],"source":["df = pd.read_json(\"dataset/constitutional_court_rulings.json\")\n","\n","sentences = list(set(df.fatti)) + list(set(df.motivi)) + list(set(df.decisione))\n","\n","new_word = []\n","count_new_word = []\n","all_words = []\n","\n","for s in tqdm(sentences):\n","    sent_text = nltk.sent_tokenize(s)\n","    for sentence in sent_text:\n","        words = parse_text(sentence)\n","        all_words += words\n","\n","        for w in words:\n","            emb = tokenizer.encode_plus(w)\n","\n","            if len(emb['input_ids']) > 3:\n","                if w not in new_word:\n","                    new_word.append(w)\n","                    count_new_word.append(1)\n","                else:\n","                    count_new_word[new_word.index(w)] += 1\n","\n","d = {\"word\": new_word, \"num\": count_new_word}\n","df = pd.DataFrame(d)\n","\n","df = df.sort_values('num', axis=0, ascending=False)\n","df = df[df['num'] > 50]"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"nT5KB2kQaXXX"},"outputs":[],"source":["ind_drop = []\n","word_add = []\n","part_of_word_to_append = []\n","count_part_of_word_to_append = []\n","\n","# Controllo parole contententi punteggiatura (es l'ambiente -> l / ambinete)\n","for i, r in df.iterrows():\n","    punct = \"\"\n","\n","    for p in string.punctuation:\n","        if p in r.word:\n","            punct = p\n","            break\n","\n","    if punct != \"\":\n","        ind_drop.append(i)\n","        arr = r.word.split(punct)\n","        for el in arr:\n","            if len(tokenizer.encode_plus(el)['input_ids']) > 3:\n","                part_of_word_to_append.append(el)\n","                count_part_of_word_to_append.append(r.num)  "]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1653552407724,"user":{"displayName":"NICOLA ARICI","userId":"02667990344535794481"},"user_tz":-120},"id":"8l3l1VsDbey7","outputId":"cc8a2a39-2562-4352-91c1-3fda008487d5"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>word</th>\n","      <th>num</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>legittimità</td>\n","      <td>659</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>proc</td>\n","      <td>648</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>cassazione</td>\n","      <td>598</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>civ</td>\n","      <td>396</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>processuali</td>\n","      <td>384</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>140</th>\n","      <td>omesso</td>\n","      <td>53</td>\n","    </tr>\n","    <tr>\n","      <th>141</th>\n","      <td>enpals</td>\n","      <td>53</td>\n","    </tr>\n","    <tr>\n","      <th>142</th>\n","      <td>indennità</td>\n","      <td>53</td>\n","    </tr>\n","    <tr>\n","      <th>143</th>\n","      <td>inps</td>\n","      <td>51</td>\n","    </tr>\n","    <tr>\n","      <th>144</th>\n","      <td>impugnazione</td>\n","      <td>51</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>145 rows × 2 columns</p>\n","</div>"],"text/plain":["             word  num\n","0     legittimità  659\n","1            proc  648\n","2      cassazione  598\n","3             civ  396\n","4     processuali  384\n","..            ...  ...\n","140        omesso   53\n","141        enpals   53\n","142     indennità   53\n","143          inps   51\n","144  impugnazione   51\n","\n","[145 rows x 2 columns]"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["df = df.drop(ind_drop)\n","df = pd.concat([df, pd.DataFrame({\"word\": part_of_word_to_append, \"num\": count_part_of_word_to_append})]).reset_index(drop=True)\n","\n","df"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":334,"status":"ok","timestamp":1653552408043,"user":{"displayName":"NICOLA ARICI","userId":"02667990344535794481"},"user_tz":-120},"id":"rYZA8e74J34y","outputId":"120cad7f-58db-48f6-e233-6016fd979d62"},"outputs":[{"name":"stdout","output_type":"stream","text":["#tokens in tokenizer: 31102\n","Appending new tokens...\n","#tokens in tokenizer: 31224\n","\n","Modifica Embedding Matrix\n"]},{"data":{"text/plain":["Embedding(31224, 768)"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["print(\"#tokens in tokenizer:\", len(tokenizer))\n","print(\"Appending new tokens...\")\n","tokenizer.add_tokens(df.word.tolist())\n","print(\"#tokens in tokenizer:\", len(tokenizer))\n","\n","print()\n","print(\"Modifica Embedding Matrix\")\n","model.resize_token_embeddings(len(tokenizer))"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"ZhAAimVNFZSk"},"outputs":[],"source":["tokenizer.save_pretrained('models/LawBERT-IT')\n","model.save_pretrained('models/LawBERT-IT')"]},{"cell_type":"markdown","metadata":{"id":"_GGnsM_0e2u7"},"source":["# ***Addestramento nuove parole con NSP e MLM***"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5490,"status":"ok","timestamp":1653552415097,"user":{"displayName":"NICOLA ARICI","userId":"02667990344535794481"},"user_tz":-120},"id":"pEzEqhGswE2o","outputId":"d1591408-5255-4ecf-8b21-6b16c25e9b2b"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of BertForPreTraining were not initialized from the model checkpoint at models/LawBERT-IT and are newly initialized: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["tokenizer = BertTokenizer.from_pretrained('models/LawBERT-IT')\n","model = BertForPreTraining.from_pretrained('models/LawBERT-IT')"]},{"cell_type":"markdown","metadata":{"id":"4kUtlNnmQySO"},"source":["Estrazione delle frasi dalle risposte senza punteggio"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"NhEU8okQS1YH"},"outputs":[],"source":["def all_punct(s):\n","    for c in s:\n","        if c not in string.punctuation and c not in '123456789':\n","            return False\n","    return True"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36,"status":"ok","timestamp":1653552415099,"user":{"displayName":"NICOLA ARICI","userId":"02667990344535794481"},"user_tz":-120},"id":"0ryz2cLuXaId","outputId":"8383b4b5-150b-4e26-87c7-4bee872fb16d"},"outputs":[{"data":{"text/plain":["(702, 46200)"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["df_sent = pd.read_json(\"dataset/constitutional_court_rulings.json\")\n","\n","process = list(set(df_sent.fatti)) + list(set(df_sent.motivi)) + list(set(df_sent.decisione))\n","bag = []\n","for proc in process:\n","    bag += [s.lower() for s in proc.split('.') if s.strip() != '' and not all_punct(s)]\n","\n","len(process), len(bag)"]},{"cell_type":"markdown","metadata":{"id":"8FrW1wbnQ0md"},"source":["Preparazione dati per NSP (50% frasi consecutive, 50% no)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29,"status":"ok","timestamp":1653552415100,"user":{"displayName":"NICOLA ARICI","userId":"02667990344535794481"},"user_tz":-120},"id":"mP5Z2qo-wf5m","outputId":"5581ae1b-39f8-4103-d08f-d6856b5282bb"},"outputs":[{"data":{"text/plain":["(32171, 16060, 16111)"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["import random\n","\n","sentence_a = []\n","sentence_b = []\n","label = []\n","\n","for ans in process:\n","    sentences = [sentence for sentence in ans.split('.') if sentence != '']\n","\n","    while len(sentences) > 0:\n","\n","        if len(sentences) > 1:\n","            start = random.randint(0, len(sentences) - 2)\n","        else:\n","            start = 0\n","\n","        if len(sentences) > 1 and random.random() >= 0.49:\n","            sentence_a.append(sentences.pop(start))\n","            sentence_b.append(sentences.pop(start))\n","            label.append(0)\n","        else:\n","            sentence_a.append(sentences[start])\n","\n","            cand = bag[random.randint(0, len(bag)-1)]\n","            while cand == sentences[start]:\n","                cand = bag[random.randint(0, len(bag)-1)]\n","\n","            sentences.pop(start)\n","\n","            sentence_b.append(cand)\n","            label.append(1)\n","\n","len(label), len([l for l in label if l == 0]), len([l for l in label if l == 1])"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"F8OHVLdASSl2"},"outputs":[],"source":["# Genera gli embedding.\n","\n","inputs = tokenizer(sentence_a, sentence_b, \n","                   return_tensors='pt',\n","                   max_length=512, \n","                   truncation=True, \n","                   padding='max_length')\n","\n","inputs['next_sentence_label'] = torch.LongTensor([label]).T"]},{"cell_type":"markdown","metadata":{"id":"VNKv0TXBiDX1"},"source":["Preparazione dati per MLM (Masck del 15% dei token in input_ids)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"vM9_BL2derev"},"outputs":[],"source":["inputs['labels'] = inputs.input_ids.detach().clone()"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"2HOfmR-BpTNk"},"outputs":[],"source":["def check_added_vocab(id):\n","    for k, i in tokenizer.get_added_vocab().items():\n","        if id == i:\n","            return True\n","    return False"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":468327,"status":"ok","timestamp":1653552937234,"user":{"displayName":"NICOLA ARICI","userId":"02667990344535794481"},"user_tz":-120},"id":"CseAAYLXnwOi","outputId":"1fc0746b-fa00-4d9b-93fc-fc463d340ca9"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 32171/32171 [07:10<00:00, 74.76it/s] "]},{"name":"stdout","output_type":"stream","text":["\n","MASKED: 12.66%\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["SPECIAL_TOKENS = [101, 102, 103, 104]\n","\n","masked = 0\n","num_tokens = 0\n","\n","for emb in tqdm(inputs.input_ids):\n","    for i in range(len(emb)):\n","        if emb[i] == 0:\n","            break\n","        elif check_added_vocab(emb[i]) and random.uniform(0.0, 1.0) < 0.50:\n","            emb[i] = 104\n","            masked += 1\n","        elif emb[i] not in SPECIAL_TOKENS and random.uniform(0.0, 1.0) < 0.12:\n","            emb[i] = 104\n","            masked += 1\n","\n","        num_tokens += 1\n","\n","print(\"\\nMASKED:\", str(round(100*masked/num_tokens, 2)) + \"%\")"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"XS2HZcjBWjQw"},"outputs":[],"source":["indexes = list(range(0, len(inputs.input_ids)))\n","\n","random.seed(11)\n","random.shuffle(indexes)\n","\n","train_input_ids = []\n","train_attention = []\n","train_type = []\n","train_nsp = []\n","train_label= []\n","\n","test_input_ids = []\n","test_attention = []\n","test_type = []\n","test_nsp = []\n","test_label= []\n","\n","for i in range(len(inputs.input_ids)):\n","    if i < round(0.8*len(indexes)):\n","        train_input_ids.append(inputs.input_ids[i])\n","        train_attention.append(inputs.attention_mask[i])\n","        train_type.append(inputs.token_type_ids[i])\n","        train_nsp.append(inputs.next_sentence_label[i])\n","        train_label.append(inputs.labels[i])\n","\n","    else:\n","        test_input_ids.append(inputs.input_ids[i])\n","        test_attention.append(inputs.attention_mask[i])\n","        test_type.append(inputs.token_type_ids[i])\n","        test_nsp.append(inputs.next_sentence_label[i])\n","        test_label.append(inputs.labels[i])\n","\n","train_inputs = {'input_ids': train_input_ids, \n","                'token_type_ids': train_type, \n","                'attention_mask': train_attention, \n","                'next_sentence_label': train_nsp, \n","                'labels': train_label}\n","\n","test_inputs = {'input_ids': test_input_ids, \n","               'token_type_ids': test_type, \n","               'attention_mask': test_attention, \n","               'next_sentence_label': test_nsp, \n","               'labels': test_label}"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"muqaxznSi0Eg"},"outputs":[],"source":["class CustomDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings):\n","        self.encodings = encodings\n","\n","    def __getitem__(self, idx):\n","        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        \n","    def __len__(self):\n","        return len(self.encodings['input_ids'])"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"ubBImrsypLS0"},"outputs":[],"source":["train_dataset = CustomDataset(train_inputs)\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n","\n","\n","val_dataset= CustomDataset(test_inputs)\n","val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=8, shuffle=True)"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"v8IrLitZpOqu"},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/3218 [00:00<?, ?it/s]/home/ec2-user/.conda/envs/simatt/lib/python3.7/site-packages/ipykernel_launcher.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  \n","100%|██████████| 3218/3218 [15:56<00:00,  3.36it/s]\n","100%|██████████| 805/805 [01:14<00:00, 10.78it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 0 - Val loss: 0.7711525497560993\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3218/3218 [15:57<00:00,  3.36it/s]\n","100%|██████████| 805/805 [01:14<00:00, 10.78it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 1 - Val loss: 0.2251476455060794\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3218/3218 [15:57<00:00,  3.36it/s]\n","100%|██████████| 805/805 [01:14<00:00, 10.76it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 2 - Val loss: 0.36101895841180054\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3218/3218 [15:56<00:00,  3.36it/s]\n","100%|██████████| 805/805 [01:14<00:00, 10.78it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 3 - Val loss: 0.3442353695501371\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3218/3218 [15:56<00:00,  3.36it/s]\n","100%|██████████| 805/805 [01:14<00:00, 10.78it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 4 - Val loss: 0.3946357580206274\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3218/3218 [15:56<00:00,  3.36it/s]\n","100%|██████████| 805/805 [01:14<00:00, 10.78it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 5 - Val loss: 0.2867971424585251\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3218/3218 [15:55<00:00,  3.37it/s]\n","100%|██████████| 805/805 [01:14<00:00, 10.78it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 6 - Val loss: 0.2776728586205197\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3218/3218 [15:56<00:00,  3.37it/s]\n","100%|██████████| 805/805 [01:14<00:00, 10.78it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 7 - Val loss: 0.861450067360702\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3218/3218 [15:56<00:00,  3.36it/s]\n","100%|██████████| 805/805 [01:14<00:00, 10.78it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 8 - Val loss: 0.26131701995453677\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3218/3218 [15:55<00:00,  3.37it/s]\n","100%|██████████| 805/805 [01:14<00:00, 10.77it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch: 9 - Val loss: 0.2651150921918799\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["import torch.optim as optim\n","import copy\n","import numpy as np\n","\n","epochs = 10\n","best_loss = np.Inf\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","optim = optim.AdamW(model.parameters())\n","\n","for epoch in range(epochs):\n","    model.train()\n","\n","    for batch in tqdm(train_loader):\n","        optim.zero_grad()\n","\n","        input_ids = batch['input_ids'].to(device)\n","        token_type_ids = batch['token_type_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        next_sentence_label = batch['next_sentence_label'].to(device)\n","        labels = batch['labels'].to(device)\n","       \n","        outputs = model(input_ids, attention_mask=attention_mask,\n","                        token_type_ids=token_type_ids,\n","                        next_sentence_label=next_sentence_label,\n","                        labels=labels,\n","                        return_dict=True)\n","        \n","        loss = outputs.loss\n","        loss.backward()\n","        optim.step()\n","\n","\n","    # Validazione\n","\n","    model.eval()\n","    val_loss = 0\n","    val_loop = tqdm(val_loader, leave=True)\n","\n","    for batch in val_loop:\n","\n","        input_ids = batch['input_ids'].to(device)\n","        token_type_ids = batch['token_type_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        next_sentence_label = batch['next_sentence_label'].to(device)\n","        labels = batch['labels'].to(device)\n","\n","        val_outputs = model(input_ids, attention_mask=attention_mask,\n","                            token_type_ids=token_type_ids,\n","                            next_sentence_label=next_sentence_label,\n","                            labels=labels,\n","                            return_dict=True)\n","\n","        val_loss += outputs.loss.item()\n","\n","    val_loss /= len(train_loader)\n","\n","    if val_loss < best_loss:\n","        model_copy = copy.deepcopy(model)\n","        best_loss = val_loss\n","\n","    print('Epoch: ' + str(epoch) + \" - Val loss: \" + str(val_loss))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cBhbaJjae7Fm"},"outputs":[],"source":["tokenizer.save_pretrained('models/LawBERT-IT_trained')\n","model.save_pretrained('models/LawBERT-IT_trained')"]}],"metadata":{"colab":{"collapsed_sections":["swkQuU5_ewWt"],"provenance":[]},"kernelspec":{"display_name":"Python 3.8.13 ('arici': conda)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"vscode":{"interpreter":{"hash":"11070ee5dbfd709bcf7b8ae3553f79b79edf6c46071668e456b1aeb5c2afb594"}}},"nbformat":4,"nbformat_minor":0}
